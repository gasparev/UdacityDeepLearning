{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "#First reload the data we generated in 1_notmnist.ipynb\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "#Reformat into a shape that's more adapted to the models we're going to train:\n",
    "#  -data as a flat matrix,\n",
    "#  -labels as float 1-hot encodings.\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with beta 0.000100\n",
      "Minibatch loss at step 0: 21.909044\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 12.0%\n",
      "Minibatch loss at step 500: 1.545076\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 1000: 1.440026\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 1500: 0.949756\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 2000: 1.063383\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 2500: 1.028002\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 3000: 1.027636\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 79.2%\n",
      "Test accuracy: 86.8%\n",
      "################################################\n",
      "Initialized with beta 0.000127\n",
      "Minibatch loss at step 0: 19.682852\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 11.3%\n",
      "Minibatch loss at step 500: 1.495443\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 1000: 1.327077\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 1500: 0.972313\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 2000: 0.967094\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 2500: 1.091696\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 3000: 0.989659\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 79.6%\n",
      "Test accuracy: 87.0%\n",
      "################################################\n",
      "Initialized with beta 0.000162\n",
      "Minibatch loss at step 0: 18.969152\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 15.0%\n",
      "Minibatch loss at step 500: 1.965081\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 75.1%\n",
      "Minibatch loss at step 1000: 1.828281\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 1500: 0.934739\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 2000: 1.042336\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 2500: 1.027861\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 3000: 1.052798\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 79.9%\n",
      "Test accuracy: 87.2%\n",
      "################################################\n",
      "Initialized with beta 0.000207\n",
      "Minibatch loss at step 0: 21.597820\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 8.9%\n",
      "Minibatch loss at step 500: 1.747119\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 1000: 1.653322\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 1500: 1.056223\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 2000: 1.081847\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 2500: 1.044433\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 3000: 1.113482\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 80.1%\n",
      "Test accuracy: 87.1%\n",
      "################################################\n",
      "Initialized with beta 0.000264\n",
      "Minibatch loss at step 0: 17.509939\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 11.4%\n",
      "Minibatch loss at step 500: 1.477142\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 1000: 2.000791\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 1500: 1.077230\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 2000: 1.018806\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 2500: 1.173609\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 3000: 0.940422\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 80.3%\n",
      "Test accuracy: 87.7%\n",
      "################################################\n",
      "Initialized with beta 0.000336\n",
      "Minibatch loss at step 0: 18.732416\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 12.1%\n",
      "Minibatch loss at step 500: 1.975853\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 1000: 1.921831\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 1500: 0.955045\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 2000: 0.998118\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 2500: 1.079059\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 3000: 0.988207\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 80.5%\n",
      "Test accuracy: 87.3%\n",
      "################################################\n",
      "Initialized with beta 0.000428\n",
      "Minibatch loss at step 0: 18.103857\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 10.6%\n",
      "Minibatch loss at step 500: 1.795109\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 1000: 1.783510\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 1500: 1.099608\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 2000: 1.048880\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 2500: 1.031922\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 3000: 0.926425\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 80.5%\n",
      "Test accuracy: 88.2%\n",
      "################################################\n",
      "Initialized with beta 0.000546\n",
      "Minibatch loss at step 0: 20.557903\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 9.1%\n",
      "Minibatch loss at step 500: 2.177598\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 75.9%\n",
      "Minibatch loss at step 1000: 1.685206\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 1500: 1.104991\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 2000: 0.990525\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 2500: 0.909526\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 3000: 0.888813\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.3%\n",
      "Test accuracy: 88.3%\n",
      "################################################\n",
      "Initialized with beta 0.000695\n",
      "Minibatch loss at step 0: 15.632551\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 16.8%\n",
      "Minibatch loss at step 500: 2.406168\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 1000: 1.819027\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 1500: 1.034300\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 2000: 0.968759\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 2500: 0.947154\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 3000: 0.838939\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.2%\n",
      "Test accuracy: 88.6%\n",
      "################################################\n",
      "Initialized with beta 0.000886\n",
      "Minibatch loss at step 0: 23.293941\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 12.8%\n",
      "Minibatch loss at step 500: 2.374228\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 1000: 1.567544\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 1500: 1.042298\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 2000: 0.871547\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 2500: 0.850363\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 3000: 0.826588\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.9%\n",
      "Test accuracy: 89.1%\n",
      "################################################\n",
      "Initialized with beta 0.001129\n",
      "Minibatch loss at step 0: 24.579525\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 9.1%\n",
      "Minibatch loss at step 500: 2.820082\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 1000: 1.874309\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 1500: 0.919905\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 2000: 0.793816\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 2500: 0.853865\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 3000: 0.782582\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.9%\n",
      "Test accuracy: 89.1%\n",
      "################################################\n",
      "Initialized with beta 0.001438\n",
      "Minibatch loss at step 0: 19.311485\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 15.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 500: 2.577095\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 1000: 1.532017\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 1500: 0.776887\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 2000: 0.713898\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 2500: 0.765069\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 3000: 0.731255\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.0%\n",
      "Test accuracy: 89.1%\n",
      "################################################\n",
      "Initialized with beta 0.001833\n",
      "Minibatch loss at step 0: 23.264885\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 10.3%\n",
      "Minibatch loss at step 500: 2.497534\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 1000: 1.441028\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 1500: 0.688013\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 2000: 0.646722\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 2500: 0.743047\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 3000: 0.714292\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.0%\n",
      "Test accuracy: 89.1%\n",
      "################################################\n",
      "Initialized with beta 0.002336\n",
      "Minibatch loss at step 0: 25.706701\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 13.1%\n",
      "Minibatch loss at step 500: 2.648232\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 1000: 1.285435\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 1500: 0.634055\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2000: 0.627185\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 2500: 0.736665\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 3000: 0.717027\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.9%\n",
      "Test accuracy: 89.0%\n",
      "################################################\n",
      "Initialized with beta 0.002976\n",
      "Minibatch loss at step 0: 25.396557\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 14.3%\n",
      "Minibatch loss at step 500: 2.332159\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 1000: 1.090086\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 1500: 0.571104\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2000: 0.598130\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 2500: 0.737117\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 3000: 0.723896\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 89.0%\n",
      "################################################\n",
      "Initialized with beta 0.003793\n",
      "Minibatch loss at step 0: 29.051443\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 500: 2.066490\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 1000: 0.943410\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 1500: 0.532952\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2000: 0.604809\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 2500: 0.747503\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 3000: 0.733809\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.7%\n",
      "Test accuracy: 88.9%\n",
      "################################################\n",
      "Initialized with beta 0.004833\n",
      "Minibatch loss at step 0: 32.019165\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 9.1%\n",
      "Minibatch loss at step 500: 1.595760\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 1000: 0.839595\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 1500: 0.527566\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2000: 0.614039\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2500: 0.759285\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 3000: 0.745978\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.6%\n",
      "Test accuracy: 88.8%\n",
      "################################################\n",
      "Initialized with beta 0.006158\n",
      "Minibatch loss at step 0: 37.243767\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 13.4%\n",
      "Minibatch loss at step 500: 1.237118\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 1000: 0.791670\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 1500: 0.538290\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2000: 0.627641\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2500: 0.772792\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 3000: 0.759935\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.6%\n",
      "Test accuracy: 88.8%\n",
      "################################################\n",
      "Initialized with beta 0.007848\n",
      "Minibatch loss at step 0: 43.413425\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 16.1%\n",
      "Minibatch loss at step 500: 0.926653\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 1000: 0.788817\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 1500: 0.552372\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2000: 0.643382\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 2500: 0.788207\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 3000: 0.775782\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.6%\n",
      "Test accuracy: 88.7%\n",
      "################################################\n",
      "Initialized with beta 0.010000\n",
      "Minibatch loss at step 0: 49.585617\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 8.4%\n",
      "Minibatch loss at step 500: 0.745465\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 1000: 0.801065\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 1500: 0.569989\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 2000: 0.661559\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 2500: 0.805596\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 3000: 0.793747\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 88.5%\n",
      "################################################\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEMCAYAAAAoB2Y1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c60725dc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Problem 1-a\n",
    "#Introduce and tune L2 regularization for both logistic and neural network models.\n",
    "#Let's now switch to stochastic gradient descent training instead, which is much faster.\n",
    "\n",
    "batch_size = 128\n",
    "beta_range=np.logspace(-4, -2, 20)\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_l2=tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  vars   = tf.trainable_variables() \n",
    "  lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in vars if 'biases' not in v.name ]) * beta_l2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))+lossL2\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "    \n",
    "    \n",
    "num_steps = 3001\n",
    "accuracy_test=[]\n",
    "for beta in beta_range:\n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      print(\"Initialized with beta %f\" %beta)\n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_l2 : beta}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "      test_validation_score=accuracy(test_prediction.eval(), test_labels)\n",
    "      print(\"Test accuracy: %.1f%%\" % test_validation_score)\n",
    "      print(\"################################################\")\n",
    "      accuracy_test.append(test_validation_score)\n",
    "best_beta=beta_range[np.argmax(accuracy_test)]\n",
    "accuracy_max=max(accuracy_test)\n",
    "\n",
    "plt.semilogx(beta_range, accuracy_test)\n",
    "plt.grid(True)\n",
    "plt.title('Accuracy dependance on beta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with beta 0.000100\n",
      "Minibatch loss at step 0: 704.940674\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 34.1%\n",
      "Minibatch loss at step 500: 197.624557\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 1000: 116.306084\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 1500: 69.253609\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 2000: 41.397430\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 2500: 25.382423\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 3000: 15.543882\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.5%\n",
      "Test accuracy: 92.9%\n",
      "###################################\n",
      "Initialized with beta 0.000127\n",
      "Minibatch loss at step 0: 641.424805\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 35.3%\n",
      "Minibatch loss at step 500: 204.560699\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 1000: 118.050323\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 1500: 69.411865\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 2000: 41.537140\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 2500: 25.523457\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 3000: 15.586817\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.3%\n",
      "Test accuracy: 92.9%\n",
      "###################################\n",
      "Initialized with beta 0.000162\n",
      "Minibatch loss at step 0: 774.329224\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 31.4%\n",
      "Minibatch loss at step 500: 202.452820\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 1000: 116.984055\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 1500: 69.226425\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 2000: 41.574097\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 2500: 25.336882\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 3000: 15.577938\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.4%\n",
      "Test accuracy: 93.5%\n",
      "###################################\n",
      "Initialized with beta 0.000207\n",
      "Minibatch loss at step 0: 620.827637\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 21.3%\n",
      "Minibatch loss at step 500: 196.863831\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 1000: 115.796898\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 1500: 69.093788\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 2000: 41.480827\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 2500: 25.368301\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 3000: 15.519280\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.5%\n",
      "Test accuracy: 92.8%\n",
      "###################################\n",
      "Initialized with beta 0.000264\n",
      "Minibatch loss at step 0: 643.009521\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 26.7%\n",
      "Minibatch loss at step 500: 204.378647\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 1000: 116.306717\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 1500: 68.988838\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 2000: 41.737209\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 2500: 25.478943\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 3000: 15.568288\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.3%\n",
      "Test accuracy: 92.7%\n",
      "###################################\n",
      "Initialized with beta 0.000336\n",
      "Minibatch loss at step 0: 642.521240\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 30.4%\n",
      "Minibatch loss at step 500: 206.308380\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 1000: 117.653603\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 1500: 69.312958\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 2000: 41.672607\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 2500: 25.352064\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 3000: 15.542271\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.2%\n",
      "Test accuracy: 92.8%\n",
      "###################################\n",
      "Initialized with beta 0.000428\n",
      "Minibatch loss at step 0: 666.719482\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 22.9%\n",
      "Minibatch loss at step 500: 198.731018\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 1000: 116.825775\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 1500: 69.180702\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 2000: 41.799294\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 2500: 25.481415\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 3000: 15.651290\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.5%\n",
      "Test accuracy: 93.0%\n",
      "###################################\n",
      "Initialized with beta 0.000546\n",
      "Minibatch loss at step 0: 667.632935\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 32.7%\n",
      "Minibatch loss at step 500: 200.233170\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 1000: 116.780800\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 1500: 69.075699\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 2000: 41.755283\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 2500: 25.418467\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 3000: 15.559353\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.1%\n",
      "Test accuracy: 92.9%\n",
      "###################################\n",
      "Initialized with beta 0.000695\n",
      "Minibatch loss at step 0: 632.044495\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 27.1%\n",
      "Minibatch loss at step 500: 198.394440\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 1000: 115.267326\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 1500: 68.936295\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 2000: 41.499290\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 2500: 25.240696\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 3000: 15.571784\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.2%\n",
      "Test accuracy: 92.9%\n",
      "###################################\n",
      "Initialized with beta 0.000886\n",
      "Minibatch loss at step 0: 744.040894\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 32.9%\n",
      "Minibatch loss at step 500: 201.058319\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 1000: 115.578484\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 1500: 69.583084\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 2000: 41.725346\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 2500: 25.456585\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 3000: 15.614933\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.7%\n",
      "Test accuracy: 92.8%\n",
      "###################################\n",
      "Initialized with beta 0.001129\n",
      "Minibatch loss at step 0: 622.254761\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 31.3%\n",
      "Minibatch loss at step 500: 197.623596\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 1000: 116.580307\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 1500: 69.583298\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 2000: 41.737347\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 2500: 25.398045\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 3000: 15.623496\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.4%\n",
      "Test accuracy: 93.2%\n",
      "###################################\n",
      "Initialized with beta 0.001438\n",
      "Minibatch loss at step 0: 625.748169\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 27.4%\n",
      "Minibatch loss at step 500: 199.840576\n",
      "Minibatch accuracy: 81.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 1000: 115.546242\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 1500: 69.596581\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 2000: 41.569412\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 2500: 25.430202\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 3000: 15.584491\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.3%\n",
      "Test accuracy: 93.0%\n",
      "###################################\n",
      "Initialized with beta 0.001833\n",
      "Minibatch loss at step 0: 647.863159\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 32.7%\n",
      "Minibatch loss at step 500: 197.737137\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 1000: 116.238586\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 1500: 69.158440\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 2000: 41.520481\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 2500: 25.380840\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 3000: 15.621249\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.6%\n",
      "Test accuracy: 93.0%\n",
      "###################################\n",
      "Initialized with beta 0.002336\n",
      "Minibatch loss at step 0: 605.688599\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 24.7%\n",
      "Minibatch loss at step 500: 201.282394\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 1000: 116.413597\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 1500: 68.991234\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 2000: 41.658276\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 2500: 25.386866\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 3000: 15.552536\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.7%\n",
      "Test accuracy: 93.1%\n",
      "###################################\n",
      "Initialized with beta 0.002976\n",
      "Minibatch loss at step 0: 633.586792\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 24.9%\n",
      "Minibatch loss at step 500: 199.198456\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 1000: 116.069427\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 1500: 68.964928\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 2000: 41.637089\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 2500: 25.471289\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 3000: 15.551948\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.1%\n",
      "Test accuracy: 92.6%\n",
      "###################################\n",
      "Initialized with beta 0.003793\n",
      "Minibatch loss at step 0: 623.290405\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 26.4%\n",
      "Minibatch loss at step 500: 197.873474\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 1000: 115.913300\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 1500: 69.454659\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 2000: 41.678829\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 2500: 25.356848\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 3000: 15.576861\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.5%\n",
      "Test accuracy: 93.1%\n",
      "###################################\n",
      "Initialized with beta 0.004833\n",
      "Minibatch loss at step 0: 666.674988\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 33.5%\n",
      "Minibatch loss at step 500: 202.051651\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 1000: 115.740341\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 1500: 69.521370\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 2000: 41.832775\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 2500: 25.556786\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 3000: 15.512274\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.0%\n",
      "Test accuracy: 92.8%\n",
      "###################################\n",
      "Initialized with beta 0.006158\n",
      "Minibatch loss at step 0: 633.607727\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 30.4%\n",
      "Minibatch loss at step 500: 207.740753\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 1000: 116.609238\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 1500: 69.071930\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 2000: 41.624023\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 2500: 25.280127\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 3000: 15.555675\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.6%\n",
      "Test accuracy: 92.9%\n",
      "###################################\n",
      "Initialized with beta 0.007848\n",
      "Minibatch loss at step 0: 630.432861\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 26.7%\n",
      "Minibatch loss at step 500: 197.570892\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 1000: 116.394241\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 1500: 69.070023\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 2000: 41.719688\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 2500: 25.417439\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 3000: 15.599352\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.4%\n",
      "Test accuracy: 93.0%\n",
      "###################################\n",
      "Initialized with beta 0.010000\n",
      "Minibatch loss at step 0: 666.851318\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 27.9%\n",
      "Minibatch loss at step 500: 200.281189\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 1000: 117.647537\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 1500: 69.263939\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 2000: 41.771435\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 2500: 25.367376\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 3000: 15.560470\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.8%\n",
      "Test accuracy: 92.5%\n",
      "###################################\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 119 is out of bounds for axis 0 with size 20",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-f96d31f3c5c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"###################################\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0maccuracy_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m \u001b[0mbest_beta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbeta_range\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[0maccuracy_max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 119 is out of bounds for axis 0 with size 20"
     ]
    }
   ],
   "source": [
    "#Problem 1-b\n",
    "#Introduce and tune L2 regularization for both logistic and neural network models.\n",
    "batch_size = 128\n",
    "hidden_layer=1024\n",
    "beta_range=np.logspace(-4,-2,20)\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_l2=tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_layer]))\n",
    "  biases1 = tf.Variable(tf.zeros([hidden_layer]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_layer, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  y=tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(y, weights2) + biases2\n",
    "  vars   = tf.trainable_variables() \n",
    "  lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in vars if 'biases' not in v.name ])\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))+lossL2\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  y_val=tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(y_val, weights2) + biases2)\n",
    "\n",
    "  y_test=tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(y_test, weights2) + biases2)\n",
    "    \n",
    "    \n",
    "num_steps = 3001\n",
    "for beta in beta_range:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Initialized with beta %f\" %beta)\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_l2:beta}\n",
    "            _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (step % 500 == 0):\n",
    "              print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "              print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "              print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "        test_accuracy=accuracy(test_prediction.eval(), test_labels)\n",
    "        print(\"Test accuracy: %.1f%%\" % test_accuracy)\n",
    "        print(\"###################################\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 631.048584\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 22.8%\n",
      "Minibatch loss at step 500: 190.273987\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 1000: 115.392540\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 1500: 69.980354\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 2000: 42.440010\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 2500: 25.738218\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 3000: 15.609816\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.2%\n",
      "Test accuracy: 82.8%\n"
     ]
    }
   ],
   "source": [
    "#Problem 2\n",
    "#Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "batch_size = 128\n",
    "hidden_layer=1024\n",
    "few_batch_size=batch_size*5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_layer]))\n",
    "  biases1 = tf.Variable(tf.zeros([hidden_layer]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_layer, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  y=tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(y, weights2) + biases2\n",
    "  vars   = tf.trainable_variables() \n",
    "  lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in vars if 'biases' not in v.name ]) * 0.001\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))+lossL2\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  y_val=tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(y_val, weights2) + biases2)\n",
    "\n",
    "  y_test=tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(y_test, weights2) + biases2)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "small_train_dataset=train_dataset[:few_batch_size,:]\n",
    "small_train_labels=train_labels[:few_batch_size, :]\n",
    "\n",
    "    \n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (small_train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = small_train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = small_train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 764.435303\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 34.8%\n",
      "Minibatch loss at step 500: 214.757812\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 1000: 122.681618\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 1500: 72.521393\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 2000: 43.089493\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 2500: 25.805376\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 3000: 15.811578\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 3500: 9.893163\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 4000: 6.166235\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 4500: 3.946677\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 5000: 2.607092\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 5500: 1.825555\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 6000: 1.398068\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 6500: 0.976630\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 7000: 0.992404\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 86.8%\n",
      "Test accuracy: 93.6%\n"
     ]
    }
   ],
   "source": [
    "#Problem 3-a\n",
    "#Introduce Dropout on the hidden layer of the neural network\n",
    "\n",
    "#Problem 2\n",
    "#Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "batch_size = 128\n",
    "hidden_layer=1024\n",
    "few_batch_size=batch_size*5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_layer]))\n",
    "  biases1 = tf.Variable(tf.zeros([hidden_layer]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_layer, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  y=tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  y=tf.nn.dropout(y,0.5)\n",
    "  logits = tf.matmul(y, weights2) + biases2\n",
    "  vars   = tf.trainable_variables() \n",
    "  lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in vars if 'biases' not in v.name ]) * 0.001\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))+lossL2\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  y_val=tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(y_val, weights2) + biases2)\n",
    "\n",
    "  y_test=tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(y_test, weights2) + biases2)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "num_steps = 7001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "    valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 833.454895\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 32.7%\n",
      "Minibatch loss at step 500: 192.773895\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 1000: 116.401932\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 1500: 70.758438\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 2000: 42.842384\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 2500: 25.985039\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 3000: 15.759997\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.1%\n",
      "Test accuracy: 86.2%\n"
     ]
    }
   ],
   "source": [
    "#Problem 3-b\n",
    "\n",
    "small_train_dataset=train_dataset[:few_batch_size,:]\n",
    "small_train_labels=train_labels[:few_batch_size, :]\n",
    "\n",
    "    \n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (small_train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = small_train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = small_train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.674287\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 32.3%\n",
      "Minibatch loss at step 500: 1.180261\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 1000: 1.016351\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 1500: 0.676205\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 2000: 0.587848\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 2500: 0.580716\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 3000: 0.615852\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 3500: 0.601619\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 4000: 0.490292\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 4500: 0.474580\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 5000: 0.539676\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 5500: 0.509323\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 6000: 0.581762\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 6500: 0.426958\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 7000: 0.544307\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 7500: 0.497038\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 8000: 0.583671\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 8500: 0.442825\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 9000: 0.465718\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 9500: 0.450473\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 10000: 0.422831\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 10500: 0.421770\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 11000: 0.382265\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 11500: 0.435917\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 12000: 0.505292\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 12500: 0.411014\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 13000: 0.561191\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 13500: 0.433046\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 14000: 0.463703\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 14500: 0.543676\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 15000: 0.433782\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.6%\n",
      "Test accuracy: 96.1%\n"
     ]
    }
   ],
   "source": [
    "#Problem 4\n",
    "#Try to get the best performance you can using a multi-layer model! The best is 97.1%.\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "hidden_layer_1=1024\n",
    "hidden_layer_2= 500\n",
    "hidden_layer_3=100\n",
    "\n",
    "keep_prob_1=0.8\n",
    "keep_prob_2=0.8\n",
    "keep_prob_3=0.5\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "\n",
    "    # Variables.\n",
    "    W_in = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layer_1],stddev=np.sqrt(2.0 / (image_size*image_size))))\n",
    "    B_in = tf.Variable(tf.zeros([hidden_layer_1]))\n",
    "    \n",
    "    W_h1 = tf.Variable(tf.truncated_normal([hidden_layer_1, hidden_layer_2],stddev=np.sqrt(2.0 / hidden_layer_1)))\n",
    "    B_h1 = tf.Variable(tf.zeros([hidden_layer_2]))\n",
    "    \n",
    "    W_h2 = tf.Variable(tf.truncated_normal([hidden_layer_2, hidden_layer_3],stddev=np.sqrt(2.0 / hidden_layer_2)))\n",
    "    B_h2 = tf.Variable(tf.zeros([hidden_layer_3]))\n",
    "    \n",
    "    W_out = tf.Variable(tf.truncated_normal([hidden_layer_3, num_labels],stddev=np.sqrt(2.0 / hidden_layer_3)))\n",
    "    B_out = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    y=tf.nn.relu(tf.matmul(tf_train_dataset, W_in) + B_in)\n",
    "    #y=tf.nn.dropout(y,keep_prob_1)\n",
    "    y1=tf.nn.relu(tf.matmul(y, W_h1) + B_h1)\n",
    "    #y1=tf.nn.dropout(y1,keep_prob_2)\n",
    "    y2=tf.nn.relu(tf.matmul(y1, W_h2) + B_h2)\n",
    "    #y2=tf.nn.dropout(y2,keep_prob_3)    \n",
    "    logits = tf.matmul(y2, W_out) + B_out\n",
    "    #vars   = tf.trainable_variables() \n",
    "    #lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in vars if 'B' not in v.name ]) * 0.001\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))+0.001*(\n",
    "            tf.nn.l2_loss(W_in)+tf.nn.l2_loss(W_h1)+tf.nn.l2_loss(W_h2)+tf.nn.l2_loss(W_out))\n",
    "  \n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    y_val1 = tf.nn.relu(tf.matmul(tf_valid_dataset, W_in) + B_in)\n",
    "    y_val2 = tf.nn.relu(tf.matmul(y_val1, W_h1) + B_h1)\n",
    "    y_val3 = tf.nn.relu(tf.matmul(y_val2, W_h2) + B_h2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(y_val3, W_out) + B_out)\n",
    "\n",
    "    y_test1 = tf.nn.relu(tf.matmul(tf_test_dataset, W_in) + B_in)\n",
    "    y_test2 = tf.nn.relu(tf.matmul(y_test1, W_h1) + B_h1)\n",
    "    y_test3 = tf.nn.relu(tf.matmul(y_test2, W_h2) + B_h2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(y_test3, W_out) + B_out)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "num_steps = 15001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
